{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ilias\\Anaconda3\\lib\\site-packages\\tqdm\\std.py:648: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from models import InferSent\n",
    "import torch \n",
    "import json,os,gc\n",
    "import pickle\n",
    "from time import time\n",
    "from copy import deepcopy\n",
    "import nltk\n",
    "from nltk import sent_tokenize,word_tokenize\n",
    "from nltk.corpus import wordnet,stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "#nltk.download('all')\n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "import os,re,multiprocessing,joblib\n",
    "from multiprocessing import Pool\n",
    "from collections import defaultdict\n",
    "\n",
    "number_of_articles_to_sample = 10000\n",
    "first_selection = 750\n",
    "\n",
    "\n",
    "path = 'C:\\\\Users\\Ilias\\\\OneDrive\\\\Cours\\\\Semestre 07\\\\Linguistica\\\\practica_3\\\\'\n",
    "path_text = path + 'pdf_json\\\\'\n",
    "\n",
    "l_path = len(path_text)\n",
    "\n",
    "file_names = []\n",
    "temp_file_names = os.listdir(path_text)\n",
    "file_names.extend([path_text+file_name for file_name in temp_file_names])\n",
    "len(file_names)\n",
    "\n",
    "MODEL_PATH = path+'infersent.pkl'\n",
    "params_model = {'bsize': 64, 'word_emb_dim': 300, 'enc_lstm_dim': 2048,\n",
    "                'pool_type': 'max', 'dpout_model': 0.0, 'version': 2} #because we use infersent2 pre-trained model\n",
    "\n",
    "infersent = InferSent(params_model)\n",
    "infersent.load_state_dict(torch.load(MODEL_PATH))\n",
    "\n",
    "W2V_PATH = path+'crawl-300d-2M.vec'\n",
    "infersent.set_w2v_path(W2V_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Ilias\\\\OneDrive\\\\Cours\\\\Semestre 07\\\\Linguistica\\\\practica_3\\\\pdf_json\\\\'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles = np.random.choice(len(file_names),size=number_of_articles_to_sample,replace=False)\n",
    "file_names = np.array(file_names)[articles]\n",
    "len(file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The glossary : a list of words that articles will contain, in order to keep articles of interest and to avoid pure-scientific ones\n",
    "#using our corpus and this glossary, approximatively 20% og the texts will be accepted.\n",
    "symptoms = [ #this one was taken from a notebook of the official kaggle competition https://www.kaggle.com/justmax1604/covid-text-extraction\n",
    "    \"weight loss\",\"chills\",\"shivering\",\"convulsions\",\"deformity\",\"discharge\",\"dizziness\", \"lymphopenia\", \"sneezing\",\n",
    "    \"vertigo\",\"fatigue\",\"malaise\",\"asthenia\",\"hypothermia\",\"jaundice\",\"muscle weakness\", \"chest discomfort\",\n",
    "    \"pyrexia\",\"sweats\",\"swelling\",\"swollen\",\"painful lymph node\",\"weight gain\",\"arrhythmia\", \"loss of smell\", \"loss of appetite\", \"loss of taste\",\n",
    "    \"bradycardia\",\"chest pain\",\"claudication\",\"palpitations\",\"tachycardia\",\"dry mouth\",\"epistaxis\", \"dysgeusia\", \"hypersomnia\", \"taste loss\",\n",
    "    \"halitosis\",\"hearing loss\",\"nasal discharge\", \"nasal inflammation\", \"otalgia\",\"otorrhea\",\"sore throat\",\"toothache\",\"tinnitus\", \"dysphonia\",\n",
    "    \"trismus\",\"abdominal pain\",\"fever\",\"bloating\",\"belching\",\"bleeding\",\"bloody stool\",\"melena\",\"hematochezia\", \"burning sensation in the chest\", \n",
    "    \"constipation\",\"diarrhea\",\"dysphagia\",\"dyspepsia\",\"fecal incontinence\",\"flatulence\",\"heartburn\", \"chest tightness\", \"chest pressure\",\n",
    "    \"nausea\",\"odynophagia\",\"proctalgia fugax\",\"pyrosis\",\"steatorrhea\",\"vomiting\",\"alopecia\",\"hirsutism\", \"tachypnoea\", \"nasal obstruction\",\n",
    "    \"hypertrichosis\",\"abrasion\",\"anasarca\",\"bleeding into skin\",\"petechia\",\"purpura\",\"ecchymosis\", \"bruising\", \n",
    "    \"blister\",\"edema\",\"itching\",\"laceration\",\"rash\",\"urticaria\",\"abnormal posturing\",\"acalculia\",\"agnosia\",\"alexia\",\n",
    "    \"amnesia\",\"anomia\",\"anosognosia\",\"aphasia\",\"apraxia\",\"ataxia\",\"cataplexy\",\"confusion\",\"dysarthria\", \"nasal congestion\",\n",
    "    \"dysdiadochokinesia\",\"dysgraphia\",\"hallucination\",\"headache\",\"akinesia\",\"bradykinesia\",\"akathisia\",\"athetosis\",\n",
    "    \"ballismus\",\"blepharospasm\",\"chorea\",\"dystonia\",\"fasciculation\",\"muscle cramps\",\"myoclonus\",\"opsoclonus\",\"tic\",\n",
    "    \"tremor\",\"flapping tremor\",\"insomnia\",\"loss of consciousness\",\"syncope\",\"neck stiffness\",\"opisthotonus\",\n",
    "    \"paralysis\",\"paresis\",\"paresthesia\",\"prosopagnosia\",\"somnolence\",\"abnormal vaginal bleeding\", \"neuralgia\",\n",
    "    \"vaginal bleeding in early pregnancy\", \"miscarriage\",\"vaginal bleeding in late pregnancy\",\"amenorrhea\", \"body aches\",\n",
    "    \"infertility\",\"painful intercourse\",\"pelvic pain\",\"vaginal discharge\",\"amaurosis fugax\",\"amaurosis\", \"skin lesions\",\n",
    "    \"blurred vision\",\"double vision\",\"exophthalmos\",\"mydriasis\",\"miosis\",\"nystagmus\",\"amusia\",\"anhedonia\",\n",
    "    \"anxiety\",\"apathy\",\"confabulation\",\"depression\",\"delusion\",\"euphoria\",\"homicidal ideation\",\"irritability\",\n",
    "    \"mania\",\"paranoid ideation\",\"suicidal ideation\",\"apnea\",\"hypopnea\",\"cough\",\"dyspnea\",\"bradypnea\",\"tachypnea\",\n",
    "    \"orthopnea\",\"platypnea\",\"trepopnea\",\"hemoptysis\",\"pleuritic chest pain\",\"sputum production\",\"arthralgia\",\n",
    "    \"back pain\",\"sciatica\",\"urologic\",\"dysuria\",\"hematospermia\",\"hematuria\",\"impotence\",\"polyuria\",\n",
    "    \"retrograde ejaculation\",\"strangury\",\"urethral discharge\",\"urinary frequency\",\"urinary incontinence\",\"urinary retention\", \"anosmia\", \"myalgia\", \"rhinorrhea\", \"shortness of breath\"]\n",
    "text_file = open(\"glossary.txt\", \"r\",encoding='utf-8')\n",
    "other_terms = [i[:-1] for i in text_file.readlines()]\n",
    "glossary = symptoms+other_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_content(file_path,gloss):\n",
    "    n_pertinent_words=0\n",
    "    abstract='';body_text = '';error_count = 0\n",
    "    if os.path.splitext(file_path)[1]=='.json':\n",
    "        f = open(file_path)\n",
    "        f_json = json.load(f)\n",
    "        title = f_json[\"metadata\"][\"title\"]\n",
    "        authors_json = f_json[\"metadata\"][\"authors\"]\n",
    "        authors = ''\n",
    "        for i in authors_json:\n",
    "            if (i['first']!='' or i['last']!=''):\n",
    "                authors+=i['first']+' '+i['last']+', '\n",
    "        if authors=='':\n",
    "            authors='unknown'\n",
    "        else:\n",
    "            authors = authors[:-2]\n",
    "        \n",
    "        try:\n",
    "            abstract = f_json['abstract'][0]['text']\n",
    "        except:\n",
    "            error_count+=1\n",
    "        for i in f_json['body_text']:\n",
    "            try:\n",
    "                body_text= body_text+' '+i['text']\n",
    "            except:\n",
    "                error_count+=1\n",
    "        body_text = body_text.strip()\n",
    "        for word in gloss:\n",
    "            if word in body_text:\n",
    "                n_pertinent_words+=1\n",
    "        f.close()\n",
    "        return n_pertinent_words,body_text,abstract,authors,title,error_count\n",
    "    else:\n",
    "        return n_pertinent_words,body_text,abstract,authors,title,error_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'file_name':[],'article_no':[],'n_pertinent_words':[],'body':[],'abstract':[],'authors':[],'titles':[],'error_count':[]})\n",
    "for ind in range(len(file_names)):\n",
    "    df.loc[ind,'file_name']=file_names[ind][l_path:] #this value is to change\n",
    "    df.loc[ind,'article_no']=ind\n",
    "    df.loc[ind,'n_pertinent_words'],df.loc[ind,'body'],df.loc[ind,'abstract'],df.loc[ind,'authors'],df.loc[ind,'titles'],df.loc[ind,'error_count'] = file_content(file_path=file_names[ind],gloss=glossary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(np.where(df.applymap(lambda x: x == ''))[0],inplace=True)\n",
    "df.drop(np.where(pd.isnull(df))[0],inplace=True) #nan values,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.iloc[np.argsort(df['n_pertinent_words'])[-first_selection:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- We take the 1000 documents (top 10%) that contain the most relevant words. \n",
    "# -- While varying the original corpus size, we could also use a treshold = np.quantile(df['n_pertinent_words'],0.8).\n",
    "# -- The assumption made is that documents containing a lot of pertinent words will have more relevant content.\n",
    "index = range(len(df))\n",
    "df['Index'] = index\n",
    "df['article_no'] = index\n",
    "df = df.set_index(keys='Index',drop=True)\n",
    "with open('documents.pickle', 'wb') as f:\n",
    "    pickle.dump(df, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_file = 'corpus.txt'\n",
    "sent_dict_file = 'sent.joblib.compressed'\n",
    "word_sent_no_dict_file = 'word_sent_no.joblib.compressed'\n",
    "orig_word_sent_no_dict_file = 'orig_word_sent_no.joblib.compressed'\n",
    "stopword_file = 'stopword.txt'\n",
    "## Lemmatization function\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "# 1. Init Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def get_lemmatize(sent):\n",
    "    return \" \".join([lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in word_tokenize(sent)])\n",
    "\n",
    "def fn_lemmatize(data):\n",
    "    for ind,info in tqdm(data.iterrows(),total=data.shape[0]):\n",
    "        data.loc[ind,'sentence_lemmatized'] = get_lemmatize(sent = info['sentence'])\n",
    "    return data\n",
    "## removing stopwords\n",
    "\n",
    "def words(text): \n",
    "    return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "\n",
    "def remove_stopwords(sent):\n",
    "    ## case conversion - lower case\n",
    "    word_tokens = words(text=sent)\n",
    "    #sent = sent.lower()\n",
    "    #word_tokens = word_tokenize(sent)\n",
    "    ## removing stopwords\n",
    "    filtered_sentence = \" \".join([w for w in word_tokens if not w in stopwords])\n",
    "    ## removing digits\n",
    "    filtered_sentence = re.sub(r'\\d+','',filtered_sentence)\n",
    "    ## removing multiple space\n",
    "    filtered_sentence = words(text = filtered_sentence)\n",
    "    return \" \".join(filtered_sentence)\n",
    "\n",
    "def fn_stopword(data):\n",
    "    for ind,info in tqdm(data.iterrows(),total=data.shape[0]):\n",
    "        sent = info['sentence_lemmatized']\n",
    "        data.loc[ind,'sentence_lemma_stop'] = remove_stopwords(sent)\n",
    "    return data\n",
    "\n",
    "def fn_stopword_orig(data):\n",
    "    for ind,info in tqdm(data.iterrows(),total=data.shape[0]):\n",
    "        sent = info['sentence']\n",
    "        data.loc[ind,'sentence_stop'] = remove_stopwords(sent)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 750/750 [00:20<00:00, 36.19it/s]\n"
     ]
    }
   ],
   "source": [
    "## creating sentence dictionary\n",
    "df['article'] = df['body']+' '+df['abstract']\n",
    "df['article'].fillna('',inplace=True)\n",
    "article_no_sent_dict = dict()\n",
    "for ind,info in tqdm(df.iterrows(),total=df.shape[0]):\n",
    "    sents = sent_tokenize(str(info['article']))\n",
    "    for sent in sents: #removing lines that don't contain relevant word or that are too short. Will remove a bit more than 1/3 of the sents.\n",
    "        is_ok=0\n",
    "        for word in glossary:\n",
    "            if word in sent:\n",
    "                is_ok=1\n",
    "        if is_ok==0 or len(sent)<25:\n",
    "            sents.remove(sent)\n",
    "    article_no_sent_dict[info['article_no']] = sents\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 750/750 [00:00<00:00, 48009.52it/s]\n",
      "100%|██████████| 158465/158465 [00:17<00:00, 9209.14it/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['sent.joblib.compressed']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_no_list = list();sent_list = list()\n",
    "df_sent = pd.DataFrame({'article_id':[],'sentence':[]})\n",
    "for i in tqdm(article_no_sent_dict,total=len(article_no_sent_dict)):\n",
    "    article_no_list.extend([i]*len(article_no_sent_dict[i]))\n",
    "    sent_list.extend(article_no_sent_dict[i])\n",
    "df_sent['article_id'] = article_no_list ; df_sent['sentence'] = sent_list\n",
    "df_sent['sent_no'] = list(range(df_sent.shape[0]))\n",
    "## sentence level dictionary\n",
    "sent_dict = dict()\n",
    "for ind,info in tqdm(df_sent.iterrows(),total=df_sent.shape[0]):\n",
    "    sent_dict[info['sent_no']] = info['sentence']\n",
    "sent_dict[-1] = 'NULL'\n",
    "sent_dict_file = 'sent.joblib.compressed'\n",
    "with open('document_sents.pickle', 'wb') as f:\n",
    "    pickle.dump(df_sent, f)\n",
    "joblib.dump(sent_dict,sent_dict_file, compress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "outF = open(corpus_file, \"w\",encoding=\"utf-8\")\n",
    "for line in df_sent['sentence']:\n",
    "    if len(line)>20:\n",
    " # write line to output file\n",
    "        outF.write(line)\n",
    "        outF.write(\"\\n\")\n",
    "outF.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "155635"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "sentences = open(path+\"corpus.txt\", \"r\",encoding='utf-8').readlines()\n",
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 84776(/139940) words with w2v vectors\n",
      "Vocab size : 84776\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['word_vec.joblib']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Build infersent embeddings : if they are not already made\n",
    "infersent = InferSent({'bsize': 64, 'word_emb_dim': 300, 'enc_lstm_dim': 2048,\n",
    "                'pool_type': 'max', 'dpout_model': 0.0, 'version': 2}) #because we use infersent2 pre-trained model\n",
    "infersent.load_state_dict(torch.load(path+'infersent.pkl'))\n",
    "infersent.set_w2v_path(path+'crawl-300d-2M.vec')\n",
    "infersent.build_vocab(sentences, tokenize=True) #A few seconds. Vectorizes the vocabulary.\n",
    "embeddings = infersent.encode(sentences, tokenize=True) #Up to a few hours with thousands of final-retained articles. Build vector representations of each words of each sentence.\n",
    "joblib.dump(embeddings,'embeddings.joblib', compress=True) #Pickle can't read this variable so we use joblib (maybe too big...?)\n",
    "joblib.dump(infersent.word_vec,'word_vec.joblib',compress=True)#vectorization of our vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['df_questions.joblib']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_questions = pd.DataFrame({'questions':['Is covid a disease ?','Can we die because of covid ?'],'answers':['Yes','Yes']})\n",
    "np.array(df_questions['questions'])\n",
    "embeddings_questions = infersent.encode(np.array(df_questions['questions']), tokenize=True)\n",
    "joblib.dump(embeddings_questions,'embeddings_questions.joblib', compress=True) #Pickle can't read this variable so we use joblib (maybe too big...?)\n",
    "joblib.dump(df_questions,'df_questions.joblib', compress=True) #Pickle can't read this variable so we use joblib (maybe too big...?)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def menu():\n",
    "    print('Please enter your choice :\\n\\n')\n",
    "    print('    1. Ask a question\\n')\n",
    "    print('    2. Add a question and its answer\\n')\n",
    "    print('    3. Informations\\n')\n",
    "    return(input('    4. Future updates\\n'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferSent_corpus:\n",
    "    def __init__(self,path):\n",
    "            \n",
    "            self.infersent = InferSent({'bsize': 64, 'word_emb_dim': 300, 'enc_lstm_dim': 2048,\n",
    "                'pool_type': 'max', 'dpout_model': 0.0, 'version': 2}) #because we use infersent2 pre-trained model\n",
    "            self.infersent.load_state_dict(torch.load(path+'infersent.pkl'))\n",
    "            self.infersent.set_w2v_path(path+'crawl-300d-2M.vec')\n",
    "            self.embeddings = joblib.load(path+'embeddings.joblib')\n",
    "            self.infersent.word_vec = joblib.load(path+'word_vec.joblib')\n",
    "\n",
    "            self.sentences = open(path+\"corpus.txt\", \"r\",encoding='utf-8').readlines()\n",
    "            with open(path+'documents.pickle', 'rb') as f:\n",
    "                self.df = pickle.load(f)\n",
    "            with open(path+'document_sents.pickle', 'rb') as f:\n",
    "                self.df_sent = pickle.load(f)\n",
    "                \n",
    "    def find_similar(self,sent,n,time):\n",
    "        embedded_query = self.infersent.encode([sent], tokenize=True) #query needs to be in an array\n",
    "        similarity_matrix = cosine_similarity(embedded_query, self.embeddings)\n",
    "        np.fill_diagonal(similarity_matrix, 0)\n",
    "        similarities = similarity_matrix[0]\n",
    "        if n == 1:\n",
    "            return [np.argmax(similarities)]\n",
    "        elif n is not None:\n",
    "            return np.flip(similarities.argsort()[(-n*time):len(similarities)-(n*(time-1))][::1])\n",
    "\n",
    "    def article_inf_from_sent(self,art_no):\n",
    "        #sent=str(sent)\n",
    "        #if sent[-1:]=='\\n':\n",
    "        #    sent = sent[:-1] #[:-1] we remove \\n char due to encoding by lines\n",
    "        \n",
    "        article = self.df[self.df['article_no']==art_no]\n",
    "        authors = article['authors']\n",
    "        text = article['body']\n",
    "        abstract = article['abstract']\n",
    "        title = article['titles']\n",
    "        return authors,text,abstract,title\n",
    "    \n",
    "    def proposal(self,sent,n,time=1):\n",
    "        \n",
    "        print(('\\n Results of page '+str(time) + ' : \\n'))\n",
    "        for i in self.find_similar(sent,n,time):\n",
    "            art_no = np.array(Corpus.df_sent[Corpus.df_sent['sentence'] == str(self.sentences[i])[:-1]]['article_id'])[0] #np.array to get the element as an itneger and not as a list\n",
    "            print(('Article '+str(art_no)))\n",
    "            print(self.sentences[i])\n",
    "        value = input(\"To get more info on an article, enter its number. To see more results, enter 'More'. To make another search, enter 'Menu'. To go to previous page, enter 'Previous'.\\n\\n\")\n",
    "        if value=='More' or value=='more':\n",
    "            self.proposal(sent,n,time+1)\n",
    "            return()\n",
    "        if value=='Menu' or value=='menu':\n",
    "            menu()\n",
    "            return()\n",
    "        if value=='Previous' or value=='previous' or value=='p':\n",
    "            if(time>1):\n",
    "                self.proposal(sent,n,time-1)\n",
    "                return()\n",
    "            else:\n",
    "                print('\\nYou where to the first page. You will be sent to the menu.')\n",
    "                menu()\n",
    "                return()             \n",
    "\n",
    "        try:\n",
    "            self.print_infos(int(value))\n",
    "        except:\n",
    "            print('Error. You need to enter a valid option or number.\\nYou were sent back to the menu.')\n",
    "            menu()\n",
    "        return()\n",
    "    def print_infos(self,art_no):\n",
    "        authors,text,abstract,title = self.article_inf_from_sent(art_no)\n",
    "        print('\\nArticle n°' + str(art_no) +'.\\n' + 'Authors : ' + np.array(authors)[0] + '.\\n' + 'Title : ' + np.array(title)[0] + '\\n\\n' + 'Abstract : ' + np.array(abstract)[0])\n",
    "        value = input(\"\\nTo read the article's body, enter 'read'. To make another search, enter 'menu'.\\n\\n\")\n",
    "        if value=='read' or value=='Read' or value=='r':\n",
    "            print(np.array(text)[0] + '\\n\\n')\n",
    "            menu()\n",
    "            return()\n",
    "        elif value=='Menu' or value=='menu':\n",
    "            menu()\n",
    "        else:\n",
    "            print('Youre choice where not valid. You were sent back to the menu')\n",
    "            menu\n",
    "        return()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Corpus_questions:\n",
    "\n",
    "    def __init__(self,infersent_corpus):\n",
    "        self.infersent_corpus=infersent_corpus\n",
    "        self.embeddings_questions = joblib.load('embeddings_questions.joblib')\n",
    "        self.df_questions = joblib.load('df_questions.joblib')\n",
    "        \n",
    "    def process_query(self,query):\n",
    "        ind = self.find_similar_question(query,1)\n",
    "        value = input('\\nThis question is not recognized by the database.\\nDid you mean : ' + np.array(df_questions['questions'].loc[ind])[0] + '(yes/no)\\n')\n",
    "        if value=='yes' or value=='y' or value=='Yes' or value=='Y':\n",
    "            print('Answer : ' + np.array(df_questions['answers'].loc[ind])[0])\n",
    "            menu()\n",
    "            return()\n",
    "        if value=='no' or value=='No' or value=='n' or value=='N':\n",
    "            value = input('You could find your answer by exploring our search engine. Would you like to do it ? (yes/no)\\n')\n",
    "            if value=='no' or value=='No':\n",
    "                print('You were sent to the menu\\n\\n')\n",
    "                menu()\n",
    "                return()\n",
    "            if value=='yes' or value=='Yes' or value=='y':\n",
    "                self.infersent_corpus.proposal(query,5)\n",
    "                return()\n",
    "            print('Invalid choice. You were sent to the menu\\n\\n')\n",
    "            menu()\n",
    "            return()\n",
    "\n",
    "    def find_similar_question(self,sent,n):\n",
    "            embedded_query = self.infersent_corpus.infersent.encode([sent], tokenize=True) #query needs to be in an array\n",
    "            similarity_matrix = cosine_similarity(embedded_query, embeddings_questions)\n",
    "            np.fill_diagonal(similarity_matrix, 0)\n",
    "            similarities = similarity_matrix[0]\n",
    "            if n == 1:\n",
    "                return [np.argmax(similarities)]\n",
    "            elif n is not None:\n",
    "                return np.flip(similarities.argsort()[-n:][::1])\n",
    "\n",
    "    def add_question_answer(self):\n",
    "        #We won't check if the question is already entered. Actually, the more forms of the same question we have, the better it would be for InferSent.\n",
    "        value = input('\\nEnter your question.\\n') \n",
    "        value2 = input('\\nPlease enter your answer.\\n')\n",
    "        self.df_questions.loc[len(df_questions)] = list([value,value2])\n",
    "        embedded_query = self.infersent_corpus.infersent.encode([value], tokenize=True)\n",
    "        self.embeddings_questions = np.append(self.embeddings_questions,embedded_query,axis=0)\n",
    "        print(self.embeddings_questions)\n",
    "        joblib.dump(self.embeddings_questions,'embeddings_questions.joblib', compress=True) #Pickle can't read this variable so we use joblib (maybe too big...?)\n",
    "        joblib.dump(self.df_questions,'df_questions.joblib', compress=True) #Pickle can't read this variable so we use joblib (maybe too big...?)\n",
    "\n",
    "\n",
    "        value = input('\\n Thanks you very much. Would you like to add another question ? (yes/no)\\n') \n",
    "        if value=='y' or value=='Y' or value=='Yes' or value=='yes':\n",
    "            self.add_question_answer()\n",
    "            return()\n",
    "        else:\n",
    "            print('\\nYou were sent back to the menu.\\n')\n",
    "            #menu()\n",
    "            return()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Welcome in Sars-engine-----\n",
      "\n",
      "\n",
      "Please enter your choice :\n",
      "\n",
      "\n",
      "    1. Ask a question\n",
      "\n",
      "    2. Add a question and its answer\n",
      "\n",
      "    3. Informations\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "    4. Future updates\n",
      " 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'4'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Corpus = InferSent_corpus(path)\n",
    "Questions = Corpus_questions(Corpus)\n",
    "print('-----Welcome in Sars-engine-----\\n\\n')\n",
    "menu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      " This question is not recognized by the database.\n",
      "Did you mean : Can we die because of covid ?(yes/no)\n",
      " yes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer : Yes\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Questions.process_query('Salut')\n",
    "Questions.add_question_answer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter your question.\n",
      " yes\n",
      "\n",
      "Please enter your answer.\n",
      " yes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.00746889 -0.12430182 -0.04490045 ... -0.0378999  -0.01124218\n",
      "  -0.03285642]\n",
      " [ 0.00746889 -0.03805197 -0.06995571 ... -0.01099967 -0.01057411\n",
      "  -0.01039709]\n",
      " [ 0.00746889 -0.14438318 -0.06280056 ... -0.10446588 -0.06125115\n",
      "  -0.04683772]]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      " Thanks you very much. Would you like to add another question ? (yes/no)\n",
      " no\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You were sent back to the menu.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Questions.add_question_answer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define YES and NO tables (if value in YES,...)\n",
    "#make articles reading readable\n",
    "#forget pickle and use only joblib\n",
    "#add weights for questions (if they are frecuently asked, they have more chance to be picked)\n",
    "#add a research over article titles\n",
    "#analyze influence of tokenize argument of infersent encoding\n",
    "#find a way to delete model_path etc from first cell \n",
    "#find the way to add the vocabulary of the mqnual corpus's questions in the infersent vocab (not that much important)\n",
    "#improve menu systems (always be able to go back, etc)\n",
    "#prefilter infersent responses by ensuring important words are present in the sentences (or add weights). use nltk to get the most important words."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
